{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tree_sitter in /usr/local/lib/python3.10/dist-packages (0.20.4)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
      "Requirement already satisfied: dpu_utils in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.0)\n",
      "Requirement already satisfied: azure-storage-blob in /usr/local/lib/python3.10/dist-packages (from dpu_utils) (12.19.0)\n",
      "Requirement already satisfied: azure-identity in /usr/local/lib/python3.10/dist-packages (from dpu_utils) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dpu_utils) (1.24.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dpu_utils) (4.66.1)\n",
      "Requirement already satisfied: SetSimilaritySearch in /usr/local/lib/python3.10/dist-packages (from dpu_utils) (1.0.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from dpu_utils) (0.1.99)\n",
      "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from dpu_utils) (1.16.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from dpu_utils) (2023.12.25)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity->dpu_utils) (1.29.6)\n",
      "Requirement already satisfied: cryptography>=2.5 in /usr/lib/python3/dist-packages (from azure-identity->dpu_utils) (3.4.8)\n",
      "Requirement already satisfied: msal<2.0.0,>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity->dpu_utils) (1.26.0)\n",
      "Requirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from azure-identity->dpu_utils) (1.1.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from azure-storage-blob->dpu_utils) (0.6.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->dpu_utils) (2.21)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from azure-core<2.0.0,>=1.23.0->azure-identity->dpu_utils) (4.0.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /usr/lib/python3/dist-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity->dpu_utils) (2.3.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.0 in /usr/local/lib/python3.10/dist-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->dpu_utils) (2.8.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.23.0->azure-identity->dpu_utils) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->azure-core<2.0.0,>=1.23.0->azure-identity->dpu_utils) (1.1.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tree_sitter import Language\n",
    "\n",
    "from language_data import LANGUAGE_METADATA\n",
    "from process import DataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'java'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataProcessor.PARSER.set_language(Language('../src/build/py-tree-sitter-languages.so', language))\n",
    "\n",
    "processor = DataProcessor(language=language,\n",
    "                          language_parser=LANGUAGE_METADATA[language]['language_parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = processor.process_dee(\"JetBrains/intellij-community\", ext=LANGUAGE_METADATA[language]['ext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205557"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Would be nice to use AST to parse the function and remove the name more nicely, but\n",
    "# not time for that\n",
    "# Also filtering out @overrides would be also good idea I think\n",
    "def obfuscate_function_java(source_code, old_name, new_name=\"\"):\n",
    "    source_code = re.sub(rf\"{old_name}\", new_name, source_code)\n",
    "    return source_code\n",
    "\n",
    "def non_class_name(name):\n",
    "    return name.split('.')[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(definitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d042cbc9e9a4b538c7b61988f43df0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/205557 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare(x):\n",
    "    function_name = non_class_name(x[\"identifier\"])\n",
    "    return {\n",
    "        \"function_name\": function_name,\n",
    "        \"source_code\": obfuscate_function_java(x[\"function\"], function_name, new_name=\"x\")\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(prepare, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356f3e9e03bf4f5d94e333d82f4200c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/205557 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7993d9ca70ba4ca491ff5897f910ed3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=8):   0%|          | 0/205557 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's keep some space for prompt + function name, we will be using 1024\n",
    "# We could go higher, but I have only 3090 and want to get the training done with higher batch size\n",
    "def remove_lengthy_examples(example):\n",
    "    return len(example[\"input_ids\"]) < 950\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "dataset = dataset.map(lambda example: tokenizer(example[\"source_code\"], padding=False))\n",
    "\n",
    "dataset = dataset.filter(remove_lengthy_examples, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def remove_duplicates(dataset: Dataset) -> Dataset:\n",
    "    # Create a set of unique source codes\n",
    "    unique_source_codes = dict()\n",
    "    for i, example in enumerate(dataset):\n",
    "        unique_source_codes[example[\"source_code\"]] = i\n",
    "\n",
    "    # Sort the unique samples by their index\n",
    "    return dataset.select(sorted(unique_source_codes.values()))\n",
    "\n",
    "\n",
    "dataset = remove_duplicates(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3962dd2fee064e3fbf36b4fb9150a73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/178628 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d417d8a523143cb854d471fc24fc2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the tempalte for CodeLLama\n",
    "# We will use sharegpt format in axolotl\n",
    "def add_messages(example):\n",
    "    return {\n",
    "        \"conversations\": [\n",
    "                    {\"from\": \"system\", \"value\": f\"Given the source code of a java function, suggest a fitting name for the function.\"},\n",
    "                     {\"from\": \"human\", \"value\": example[\"source_code\"]},\n",
    "                     {\"from\": \"gpt\", \"value\": example[\"function_name\"]}]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(add_messages, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now separate into train and test\n",
    "from datasets import DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset.select(range(len(dataset)-5000)),\n",
    "    \"test\": dataset.select(range(len(dataset)-5000, len(dataset)))\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the uneeded columns\n",
    "dataset = dataset.remove_columns(list(set(dataset[\"train\"].column_names) - set([\"conversations\", \"source_code\", \"function_name\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a920eb6d704cf5b5f5914858b748f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56f931463f34107b658e44fa1de3295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/179 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544400bdd83941fcacc45564a5017296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2b2e67fb0a452b8d958daa0ffe6aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc083991cd84ca8b97a3065a9122f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/427 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/hynky/jetbrains-community-function_name/commit/1a73a649e1d2fef328f3d66ec4c2ccedd5ca3d99', commit_message='Upload dataset', commit_description='', oid='1a73a649e1d2fef328f3d66ec4c2ccedd5ca3d99', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOu will need a hf token for this\n",
    "dataset.push_to_hub(\"hynky/jetbrains-community-function_name\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
